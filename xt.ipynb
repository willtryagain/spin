{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def create_patch(y, patch_size, stride):\n",
    "    # [bs x seq_len]\n",
    "    y_next = y.clone()\n",
    "    # append the last column stride times\n",
    "    y_next = torch.cat([y_next, y[:, -1].unsqueeze(1).repeat(1, stride)], dim=1)\n",
    "    # split into patches\n",
    "    y_next = y_next.unfold(1, patch_size, stride).to(y.device)\n",
    "    return y_next  # [bs  x num_patch  x patch_len]\n",
    "\n",
    "\n",
    "def find_num_patches(window, patch_size, stride):\n",
    "    return (window - patch_size) // stride + 2\n",
    "\n",
    "\n",
    "B = 2\n",
    "L = 10\n",
    "patch_len = 4\n",
    "stride = 2\n",
    "\n",
    "\n",
    "y = torch.rand(B, L)\n",
    "y_next = create_patch(y, patch_len, stride)\n",
    "assert y_next.shape[1] == find_num_patches(L, patch_len, stride)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 4, 3])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn  as nn\n",
    "\n",
    "embedding = nn.Embedding(10, 3)\n",
    "input = torch.LongTensor([[1, 2, 4, 5], [4, 3, 2, 9]])\n",
    "embedding(input).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.8909, 0.6685, 0.3515, 0.4457],\n",
       "        [0.3515, 0.4457, 0.6368, 0.9790],\n",
       "        [0.6368, 0.9790, 0.4218, 0.4305],\n",
       "        [0.4218, 0.4305, 0.2632, 0.2384],\n",
       "        [0.2632, 0.2384, 0.2384, 0.2384]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_next[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 4])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from icecream import ic\n",
    "from torch import nn\n",
    "from torch.nn import TransformerEncoderLayer, Linear\n",
    "class MTST_layer(nn.Module):\n",
    "    def __init__(self, patch_sizes, num_patches, strides, window_size, n_head):\n",
    "        super().__init__()\n",
    "        self.trans_layers = [\n",
    "            make_model(seq_len=seq_len, d_model=patch_size) for (seq_len, patch_size) in zip(num_patches, patch_sizes)\n",
    "        ]\n",
    "        flatten_size = (patch_sizes * num_patches).sum()\n",
    "        self.ff = Linear(flatten_size, window_size)\n",
    "        self.patch_sizes = patch_sizes\n",
    "        self.window_size = window_size\n",
    "        self.num_patches = num_patches\n",
    "        self.strides = strides\n",
    "\n",
    "    def forward(self, y):\n",
    "        outputs = []\n",
    "        bs = y.shape[0]\n",
    "        for i in range(len(self.patch_sizes)):\n",
    "            y_i = create_patch(y, self.patch_sizes[i], self.strides[i])\n",
    "            # [bs x num_patch x patch_len]\n",
    "            y_i = self.trans_layers[i](y_i)\n",
    "            y_i = y_i.flatten(start_dim=1)\n",
    "            outputs.append(y_i)\n",
    "            # flatten the dims except first\n",
    "        outputs = torch.cat(outputs)\n",
    "        y = self.ff(outputs)\n",
    "        return y\n",
    "\n",
    "def create_patch(y, patch_size, stride):\n",
    "    # [bs x seq_len]\n",
    "    y_next = y.clone()\n",
    "    # append the last column stride times\n",
    "    y_next = torch.cat([y_next, y[:, -1].unsqueeze(1).repeat(1, stride)], dim=1)\n",
    "    # split into patches\n",
    "    y_next = y_next.unfold(1, patch_size, stride).to(y.device)\n",
    "    return y_next # [bs  x num_patch  x patch_len]\n",
    "\n",
    "def find_num_patches(window, patch_size, stride):\n",
    "    return (window - patch_size) // stride + 2\n",
    "\n",
    "bs = 2\n",
    "patch_size = 16\n",
    "stride = 1\n",
    "n_head = 2\n",
    "seq_len = 100\n",
    "patch_sizes = [patch_size]\n",
    "strides = [stride]\n",
    "num_patches = [find_num_patches(seq_len, patch_sizes[i], strides[i]) for i in range(len(patch_sizes))] \n",
    "\n",
    "patch_sizes = np.array(patch_sizes)\n",
    "num_patches = np.array(num_patches)\n",
    "strides = np.array(strides)\n",
    "window_size = 4\n",
    "\n",
    "y = torch.rand(bs, seq_len)\n",
    "y_next = create_patch(y, patch_size, stride)\n",
    "y_next.shape\n",
    "layer = MTST_layer(patch_sizes, num_patches, strides, window_size, n_head)\n",
    "y = layer(y)\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.2561, 0.9682, 0.6184, 0.6864, 0.6104, 0.4142, 0.8504, 0.8215,\n",
       "          0.5809, 0.2869],\n",
       "         [0.2658, 0.6479, 0.9247, 0.6005, 0.6061, 0.8402, 0.0150, 0.3218,\n",
       "          0.7634, 0.7869],\n",
       "         [0.1375, 0.6987, 0.8741, 0.3301, 0.6945, 0.2388, 0.9418, 0.1316,\n",
       "          0.6855, 0.1273]],\n",
       "\n",
       "        [[0.6349, 0.6530, 0.5069, 0.8983, 0.4652, 0.8932, 0.4598, 0.2305,\n",
       "          0.6794, 0.4683],\n",
       "         [0.0771, 0.7707, 0.2292, 0.7859, 0.4219, 0.0658, 0.1208, 0.0491,\n",
       "          0.4933, 0.9733],\n",
       "         [0.1662, 0.0629, 0.5811, 0.7097, 0.3592, 0.4266, 0.8215, 0.6264,\n",
       "          0.5168, 0.2538]]])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from os.path import exists\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.functional import log_softmax, pad\n",
    "import math\n",
    "import copy\n",
    "import time\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "import pandas as pd\n",
    "# import altair as alt\n",
    "# from torchtext.data.functional import to_map_style_dataset\n",
    "from torch.utils.data import DataLoader\n",
    "# from torchtext.vocab import build_vocab_from_iterator\n",
    "# import torchtext.datasets as datasets\n",
    "# import spacy\n",
    "# import GPUtil\n",
    "import warnings\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "import torch.distributed as dist\n",
    "import torch.multiprocessing as mp\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "\n",
    "\n",
    "# Set to False to skip notebook execution (e.g. for debugging)\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "RUN_EXAMPLES = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some convenience helper functions used throughout the notebook\n",
    "\n",
    "\n",
    "def is_interactive_notebook():\n",
    "    return __name__ == \"__main__\"\n",
    "\n",
    "\n",
    "def show_example(fn, args=[]):\n",
    "    if __name__ == \"__main__\" and RUN_EXAMPLES:\n",
    "        return fn(*args)\n",
    "\n",
    "\n",
    "def execute_example(fn, args=[]):\n",
    "    if __name__ == \"__main__\" and RUN_EXAMPLES:\n",
    "        fn(*args)\n",
    "\n",
    "\n",
    "class DummyOptimizer(torch.optim.Optimizer):\n",
    "    def __init__(self):\n",
    "        self.param_groups = [{\"lr\": 0}]\n",
    "        None\n",
    "\n",
    "    def step(self):\n",
    "        None\n",
    "\n",
    "    def zero_grad(self, set_to_none=False):\n",
    "        None\n",
    "\n",
    "\n",
    "class DummyScheduler:\n",
    "    def step(self):\n",
    "        None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 20, 8])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import copy\n",
    "import math\n",
    "from icecream import ic\n",
    "def clones(module, N):\n",
    "    \"Produce N identical layers.\"\n",
    "    return nn.ModuleList([copy.deepcopy(module) for _ in range(N)])\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    \"Core encoder is a stack of N layers\"\n",
    "\n",
    "    def __init__(self, layer, N):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.layers = clones(layer, N)\n",
    "        self.norm = LayerNorm(layer.size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"Pass the input (and mask) through each layer in turn.\"\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return self.norm(x)\n",
    "\n",
    "class LayerNorm(nn.Module):\n",
    "    \"Construct a layernorm module (See citation for details).\"\n",
    "\n",
    "    def __init__(self, features, eps=1e-6):\n",
    "        super(LayerNorm, self).__init__()\n",
    "        self.a_2 = nn.Parameter(torch.ones(features))\n",
    "        self.b_2 = nn.Parameter(torch.zeros(features))\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean = x.mean(-1, keepdim=True)\n",
    "        std = x.std(-1, keepdim=True)\n",
    "        return self.a_2 * (x - mean) / (std + self.eps) + self.b_2\n",
    "\n",
    "\n",
    "class SublayerConnection(nn.Module):\n",
    "    \"\"\"\n",
    "    A residual connection followed by a layer norm.\n",
    "    Note for code simplicity the norm is first as opposed to last.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, size, dropout):\n",
    "        super(SublayerConnection, self).__init__()\n",
    "        self.norm = LayerNorm(size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, sublayer):\n",
    "        \"Apply residual connection to any sublayer with the same size.\"\n",
    "        return x + self.dropout(sublayer(self.norm(x)))\n",
    "\n",
    "class EncoderLayer(nn.Module):\n",
    "    \"Encoder is made up of self-attn and feed forward (defined below)\"\n",
    "\n",
    "    def __init__(self, size, self_attn, feed_forward, dropout):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.self_attn = self_attn\n",
    "        self.feed_forward = feed_forward\n",
    "        self.sublayer = clones(SublayerConnection(size, dropout), 2)\n",
    "        self.size = size\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"Follow Figure 1 (left) for connections.\"\n",
    "        x = self.sublayer[0](x, lambda x: self.self_attn(x))\n",
    "        return self.sublayer[1](x, self.feed_forward)\n",
    "    \n",
    "def attention(query, key, value, mask=None, dropout=None):\n",
    "    \"Compute 'Scaled Dot Product Attention'\"\n",
    "    d_k = query.size(-1)\n",
    "    scores = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(d_k)\n",
    "    if mask is not None:\n",
    "        scores = scores.masked_fill(mask == 0, -1e9)\n",
    "    p_attn = scores.softmax(dim=-1)\n",
    "    if dropout is not None:\n",
    "        p_attn = dropout(p_attn)\n",
    "    return torch.matmul(p_attn, value), p_attn\n",
    "\n",
    "class MultiHeadedAttention(nn.Module):\n",
    "    def __init__(self, h, d_model, attn, seq_len, dropout=0.1):\n",
    "        \"Take in model size and number of heads.\"\n",
    "        super(MultiHeadedAttention, self).__init__()\n",
    "        assert d_model % h == 0\n",
    "        # We assume d_v always equals d_k\n",
    "        self.d_k = d_model // h\n",
    "        self.h = h\n",
    "        self.linear = nn.Linear(d_model, d_model)\n",
    "        self.attn = attn(d_model, h, seq_len, dropout)\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"Implements Figure 2\"\n",
    "        x = self.attn(x)\n",
    "        return self.linear(x)\n",
    "    \n",
    "class PositionwiseFeedForward(nn.Module):\n",
    "    \"Implements FFN equation.\"\n",
    "\n",
    "    def __init__(self, d_model, d_ff, dropout=0.1):\n",
    "        super(PositionwiseFeedForward, self).__init__()\n",
    "        self.w_1 = nn.Linear(d_model, d_ff)\n",
    "        self.w_2 = nn.Linear(d_ff, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.w_2(self.dropout(self.w_1(x).relu()))\n",
    "    \n",
    "def make_model(seq_len, attn=RelativeGlobalAttention, \n",
    "    N=6, d_model=512, d_ff=2048, h=8, dropout=0.1\n",
    "):\n",
    "    \"Helper: Construct a model from hyperparameters.\"\n",
    "    c = copy.deepcopy\n",
    "    attn = MultiHeadedAttention(h, d_model, attn, seq_len, dropout)\n",
    "    ff = PositionwiseFeedForward(d_model, d_ff, dropout)\n",
    "    model = Encoder(EncoderLayer(d_model, c(attn), c(ff), dropout), N)\n",
    "\n",
    "\n",
    "    # This was important from their code.\n",
    "    # Initialize parameters with Glorot / fan_avg.\n",
    "    for p in model.parameters():\n",
    "        if p.dim() > 1:\n",
    "            nn.init.xavier_uniform_(p)\n",
    "    return model\n",
    "\n",
    "batch_size = 10\n",
    "seq_len = 20\n",
    "d_model = 8\n",
    "model = make_model(seq_len, d_model=d_model)\n",
    "\n",
    "x = torch.rand(batch_size, seq_len, d_model)\n",
    "src_mask = torch.ones(1, 1, seq_len)\n",
    "y = model(x)\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 100, 768])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "\n",
    "class RelativeGlobalAttention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, max_len=1024, dropout=0.1):\n",
    "        super().__init__()\n",
    "        d_head, remainder = divmod(d_model, num_heads)\n",
    "        if remainder:\n",
    "            raise ValueError(\n",
    "                \"incompatible `d_model` and `num_heads`\"\n",
    "            )\n",
    "        self.max_len = max_len\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.key = nn.Linear(d_model, d_model)\n",
    "        self.value = nn.Linear(d_model, d_model)\n",
    "        self.query = nn.Linear(d_model, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.Er = nn.Parameter(torch.randn(max_len, d_head))\n",
    "        self.register_buffer(\n",
    "            \"mask\", \n",
    "            torch.ones(max_len, max_len)\n",
    "            .unsqueeze(0).unsqueeze(0)\n",
    "        )\n",
    "        # self.mask.shape = (1, 1, max_len, max_len)\n",
    "\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x.shape == (batch_size, seq_len, d_model)\n",
    "        batch_size, seq_len, _ = x.shape\n",
    "        \n",
    "        if seq_len > self.max_len:\n",
    "            raise ValueError(\n",
    "                \"sequence length exceeds model capacity\"\n",
    "            )\n",
    "        \n",
    "        k_t = self.key(x).reshape(batch_size, seq_len, self.num_heads, -1).permute(0, 2, 3, 1)\n",
    "        # k_t.shape = (batch_size, num_heads, d_head, seq_len)\n",
    "        v = self.value(x).reshape(batch_size, seq_len, self.num_heads, -1).transpose(1, 2)\n",
    "        q = self.query(x).reshape(batch_size, seq_len, self.num_heads, -1).transpose(1, 2)\n",
    "        # shape = (batch_size, num_heads, seq_len, d_head)\n",
    "        \n",
    "        start = self.max_len - seq_len\n",
    "        Er_t = self.Er[start:, :].transpose(0, 1)\n",
    "        # Er_t.shape = (d_head, seq_len)\n",
    "        QEr = torch.matmul(q, Er_t)\n",
    "        # QEr.shape = (batch_size, num_heads, seq_len, seq_len)\n",
    "        Srel = self.skew(QEr)\n",
    "        # Srel.shape = (batch_size, num_heads, seq_len, seq_len)\n",
    "        \n",
    "        QK_t = torch.matmul(q, k_t)\n",
    "        # QK_t.shape = (batch_size, num_heads, seq_len, seq_len)\n",
    "        attn = (QK_t + Srel) / math.sqrt(q.size(-1))\n",
    "        mask = self.mask[:, :, :seq_len, :seq_len]\n",
    "        # mask.shape = (1, 1, seq_len, seq_len)\n",
    "        attn = attn.masked_fill(mask == 0, float(\"-inf\"))\n",
    "        # attn.shape = (batch_size, num_heads, seq_len, seq_len)\n",
    "        attn = F.softmax(attn, dim=-1)\n",
    "        out = torch.matmul(attn, v)\n",
    "        # out.shape = (batch_size, num_heads, seq_len, d_head)\n",
    "        out = out.transpose(1, 2)\n",
    "        # out.shape == (batch_size, seq_len, num_heads, d_head)\n",
    "        out = out.reshape(batch_size, seq_len, -1)\n",
    "        # out.shape == (batch_size, seq_len, d_model)\n",
    "        return self.dropout(out)\n",
    "        \n",
    "    \n",
    "    def skew(self, QEr):\n",
    "        # QEr.shape = (batch_size, num_heads, seq_len, seq_len)\n",
    "        padded = F.pad(QEr, (1, 0))\n",
    "        # padded.shape = (batch_size, num_heads, seq_len, 1 + seq_len)\n",
    "        batch_size, num_heads, num_rows, num_cols = padded.shape\n",
    "        reshaped = padded.reshape(batch_size, num_heads, num_cols, num_rows)\n",
    "        # reshaped.size = (batch_size, num_heads, 1 + seq_len, seq_len)\n",
    "        Srel = reshaped[:, :, 1:, :]\n",
    "        # Srel.shape = (batch_size, num_heads, seq_len, seq_len)\n",
    "        return Srel\n",
    "\n",
    "batch_size = 10\n",
    "\n",
    "seq_len = 100\n",
    "d_model = 768\n",
    "num_heads = 12\n",
    "\n",
    "test_in = torch.randn(batch_size, seq_len, d_model)\n",
    "l = RelativeGlobalAttention(d_model, num_heads)\n",
    "l(test_in).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-01-03 12:06:20,202 [INFO]: Generating mask with base p=0.0\n"
     ]
    }
   ],
   "source": [
    "from tsl.datasets import AirQuality, MetrLA, PemsBay\n",
    "from tsl.ops.imputation import add_missing_values\n",
    "\n",
    "def get_dataset(dataset_name: str):\n",
    "    if dataset_name.startswith(\"air\"):\n",
    "        return AirQuality(impute_nans=True, small=dataset_name[3:] == \"36\")\n",
    "    # build missing dataset\n",
    "    if dataset_name.endswith(\"_point\"):\n",
    "        p_fault, p_noise = 0.0, 0.25\n",
    "        dataset_name = dataset_name[:-6]\n",
    "    elif dataset_name.endswith(\"_block\"):\n",
    "        p_fault, p_noise = 0.0015, 0.05\n",
    "        dataset_name = dataset_name[:-6]\n",
    "    else:\n",
    "        raise ValueError(f\"Invalid dataset name: {dataset_name}.\")\n",
    "    if dataset_name == \"la\":\n",
    "        return add_missing_values(\n",
    "            MetrLA(),\n",
    "            p_fault=p_fault,\n",
    "            p_noise=p_noise,\n",
    "            min_seq=12,\n",
    "            max_seq=12 * 4,\n",
    "            seed=9101112,\n",
    "        )\n",
    "    if dataset_name == \"bay\":\n",
    "        return add_missing_values(\n",
    "            PemsBay(),\n",
    "            p_fault=p_fault,\n",
    "            p_noise=p_noise,\n",
    "            min_seq=12,\n",
    "            max_seq=12 * 4,\n",
    "            seed=56789,\n",
    "        )\n",
    "    raise ValueError(f\"Invalid dataset name: {dataset_name}.\")\n",
    "\n",
    "dataset_name = \"bay_point\"\n",
    "dataset = get_dataset(dataset_name)\n",
    "target = dataset.dataframe().values\n",
    "mask "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.4747], grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "# an Embedding module containing 10 tensors of size 3\n",
    "embedding = nn.Embedding(10, 1)\n",
    "# a batch of 2 samples of 4 indices each\n",
    "input = torch.LongTensor([0])\n",
    "embedding(input)[0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "conda create -n spin python=3.8\n",
    "conda activate spin\n",
    "conda install pytorch torchvision torchaudio pytorch-cuda=11.8 -c pytorch -c nvidia\n",
    "conda install pyg -c pyg \n",
    "pip install torch-scatter -f https://data.pyg.org/whl/torch-2.1.0+cu118.html\n",
    "pip install icecream\n",
    "pip install torch_spatiotemporal==0.1.1\n",
    "pip install pandas==1.4.4\n",
    "pip install torchmetrics==0.7.0\n",
    "pip install wandb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.3706, -0.2865,  0.7672, -1.2363],\n",
       "         [ 0.7999, -2.2472,  1.2285, -0.0145],\n",
       "         [-0.3972,  0.3705,  0.7028,  0.4143]],\n",
       "\n",
       "        [[ 1.1328, -0.9891,  0.2122,  0.7984],\n",
       "         [ 1.3582,  1.3915, -0.7817, -0.6203],\n",
       "         [ 2.2937, -0.2132,  1.4776, -0.3539]]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch \n",
    "\n",
    "a = torch.randn(2, 3, 4)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.3706, -0.2865,  0.7672, -1.2363],\n",
       "         [ 0.7999, -2.2472,  1.2285, -0.0145],\n",
       "         [-0.3972,  0.3705,  0.7028,  0.4143]],\n",
       "\n",
       "        [[ 1.1328, -0.9891,  0.2122,  0.7984],\n",
       "         [ 1.3582,  1.3915, -0.7817, -0.6203],\n",
       "         [ 2.2937, -0.2132,  1.4776, -0.3539]]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.3706,  0.7999, -0.3972],\n",
       "        [-0.2865, -2.2472,  0.3705],\n",
       "        [ 0.7672,  1.2285,  0.7028],\n",
       "        [-1.2363, -0.0145,  0.4143],\n",
       "        [ 1.1328,  1.3582,  2.2937],\n",
       "        [-0.9891,  1.3915, -0.2132],\n",
       "        [ 0.2122, -0.7817,  1.4776],\n",
       "        [ 0.7984, -0.6203, -0.3539]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = a.permute((1, 0, 2)).flatten(start_dim=1).permute((1, 0))\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([8, 3]),)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.shape, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 3, 1])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.unsqueeze(2).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(True)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch = 2\n",
    "l = 3\n",
    "n = 4\n",
    "\n",
    "c = b.unflatten(0, (batch, n)).permute((0, 2, 1))\n",
    "(a == c).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: invalid device ordinal\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m index \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mLongTensor([\u001b[38;5;241m2\u001b[39m])\n\u001b[1;32m      5\u001b[0m emb(index)[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m----> 6\u001b[0m a \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrandn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcuda:1\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m a \u001b[38;5;241m+\u001b[39m emb(index)[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: invalid device ordinal\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "emb = torch.nn.Embedding(4, 1)\n",
    "index = torch.LongTensor([2])\n",
    "emb(index)[0][0]\n",
    "a = torch.randn(3, 4).to(\"cuda:1\")\n",
    "a + emb(index)[0][0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "lin = nn"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spin",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
